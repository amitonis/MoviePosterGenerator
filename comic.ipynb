{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea62266-6c53-4d4e-aebc-e8bd981fa5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.5.1+cu121)\n",
      "    Python  3.11.9 (you have 3.11.9)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amitm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875c66f8bb214fcea128e967c1de8f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\amitm\\.cache\\huggingface\\hub\\models--ogkalu--Comic-Diffusion\\snapshots\\ff684f581ab24e094e2055d9422e9ee076d139a8\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\amitm\\.cache\\huggingface\\hub\\models--ogkalu--Comic-Diffusion\\snapshots\\ff684f581ab24e094e2055d9422e9ee076d139a8\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch C:\\Users\\amitm\\.cache\\huggingface\\hub\\models--ogkalu--Comic-Diffusion\\snapshots\\ff684f581ab24e094e2055d9422e9ee076d139a8\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\amitm\\.cache\\huggingface\\hub\\models--ogkalu--Comic-Diffusion\\snapshots\\ff684f581ab24e094e2055d9422e9ee076d139a8\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Free up GPU memory before loading the model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load Comic Diffusion Model with Optimized Settings\n",
    "model_id = \"ogkalu/Comic-Diffusion\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for lower memory\n",
    "    safety_checker=None,  # Disable safety checker for performance (Use responsibly)\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Optimize for Low VRAM\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "print(\" Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9e3964-d970-4fd7-96b0-8ccdbdf3e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Enter a 25+ word movie description:  Capt. Jack Sparrow arrives at Port Royal in the Caribbean without a ship or crew. His timing is inopportune, however, because later that evening the town is besieged by a pirate ship. The pirates kidnap the governor's daughter, Elizabeth, who's in possession of a valuable coin that is linked to a curse that has transformed the pirates into the undead. A gallant blacksmith in love with Elizabeth allies with Sparrow in pursuit of the pirates.\n"
     ]
    }
   ],
   "source": [
    "#  Get user input\n",
    "movie_description = input(\"ðŸŽ¬ Enter a 25+ word movie description: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee65a88-cf5c-4b4b-975d-98201869eaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (94 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['gallant blacksmith in love with elizabeth allies with sparrow in pursuit of the pirates .']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec10234516c4c3e84b7dd7b2f0c21bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Poster generated in 143.06 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Generate Poster\n",
    "start_time = time.time()\n",
    "\n",
    "poster = pipe(\n",
    "    movie_description, \n",
    "    width=768,  # Higher quality resolution\n",
    "    height=1024, \n",
    "    num_inference_steps=50  # High-quality generation\n",
    ").images[0]\n",
    "\n",
    "# Save & Display Poster\n",
    "poster_path = \"movie_poster.png\"\n",
    "poster.save(poster_path)\n",
    "poster.show()\n",
    "\n",
    "print(f\"Movie Poster generated in {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edaa97e0-4771-4ee0-883b-fae4dd1d3a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4All Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "# Download & Load the Model (Replace with a valid model name)\n",
    "gpt = GPT4All(\"mistral-7b-openorca.Q4_0.gguf\", allow_download=True)\n",
    "\n",
    "# Verify installation\n",
    "print(\"GPT4All Model Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095791ed-c925-4ab5-87f5-eaa43c2d5a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬Generated Movie Title: \"Curse of the Undead Pirate\"\n",
      "ðŸŽ¬Generated Tagline: \"All Hands on Deck for an Unholy Voyage!\"\n",
      "Movie Poster saved as final_movie_poster.png\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load GPT4All Model\n",
    "gpt = GPT4All(\"mistral-7b-openorca.Q4_0.gguf\")\n",
    "\n",
    "# Generate a Movie Title & Tagline\n",
    "prompt = f\"Generate a compelling movie title and tagline for this plot: {movie_description}\\n\\nTitle: \"\n",
    "response = gpt.generate(prompt)\n",
    "\n",
    "# Extract Title & Tagline\n",
    "lines = response.strip().split(\"\\n\")\n",
    "movie_title = lines[0].replace(\"Title:\", \"\").strip() if len(lines) > 0 else \"Unknown Title\"\n",
    "tagline = lines[1].replace(\"Tagline:\", \"\").strip() if len(lines) > 1 else \"Unknown Tagline\"\n",
    "\n",
    "print(f\"ðŸŽ¬Generated Movie Title: {movie_title}\")\n",
    "print(f\"ðŸŽ¬Generated Tagline: {tagline}\")\n",
    "\n",
    "# Load the Generated Movie Poster\n",
    "poster_path = \"movie_poster.png\"  # Path to the generated poster\n",
    "poster = Image.open(poster_path)\n",
    "\n",
    "# Convert Image to Editable Format\n",
    "draw = ImageDraw.Draw(poster)\n",
    "width, height = poster.size\n",
    "\n",
    "# Load Font (Uses default if arial.ttf not found)\n",
    "try:\n",
    "    title_font = ImageFont.truetype(\"arial.ttf\", 80)  # Large font for title\n",
    "    tagline_font = ImageFont.truetype(\"arial.ttf\", 40)  # Smaller font for tagline\n",
    "except:\n",
    "    title_font = ImageFont.load_default()\n",
    "    tagline_font = ImageFont.load_default()\n",
    "\n",
    "title_x = width // 2\n",
    "title_y = int(height * 0.1) \n",
    "\n",
    "tagline_x = width // 2\n",
    "tagline_y = int(height * 0.85)  \n",
    "\n",
    "# Add AI-Generated Title & Tagline\n",
    "draw.text((title_x, title_y), movie_title, font=title_font, fill=\"white\", anchor=\"mm\")\n",
    "draw.text((tagline_x, tagline_y), tagline, font=tagline_font, fill=\"white\", anchor=\"mm\")\n",
    "\n",
    "# Save Final Poster with AI-Generated Text\n",
    "output_path = \"final_movie_poster.png\"\n",
    "poster.save(output_path)\n",
    "\n",
    "print(f\"Movie Poster saved as {output_path}\")\n",
    "poster.show()  # Display final poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e91df-a2a5-4705-9a88-f44ddd779bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "import openai\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Replace with your API key or model integration method\n",
    "OPENAI_API_KEY = \"your-api-key-here\"\n",
    "\n",
    "def generate_image(prompt):\n",
    "    response = openai.Image.create(\n",
    "        prompt=prompt,\n",
    "        n=1,\n",
    "        size=\"1024x1024\"\n",
    "    )\n",
    "    return response[\"data\"][0][\"url\"]\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    if request.method == \"POST\":\n",
    "        prompt = request.form[\"prompt\"]\n",
    "        image_url = generate_image(prompt)\n",
    "        return render_template(\"index.html\", image_url=image_url, prompt=prompt)\n",
    "    return render_template(\"index.html\", image_url=None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b09c20-050b-4eb7-874b-e9dbb7d12a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edebb4c7-9693-452d-aa4b-0a4e447d5ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.32.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5.2)\n",
      "Collecting flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (0.29.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from diffusers) (11.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from flask) (3.1.4)\n",
      "Collecting itsdangerous>=2.2 (from flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from flask) (8.1.8)\n",
      "Collecting blinker>=1.9 (from flask)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->diffusers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amitm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->diffusers) (2025.1.31)\n",
      "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, blinker, flask\n",
      "Successfully installed blinker-1.9.0 flask-3.1.0 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1c83dc-51ef-4f6d-98ce-e9d737963555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.5.1+cu121)\n",
      "    Python  3.11.9 (you have 3.11.9)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amitm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bed09ac58fe44298286669a80f4f3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ee338950024a18a76fd3268802b809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  29%|##9       | 357M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30590d56cda947008cd6c89461ed4f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   3%|3         | 115M/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752c95cf28db429cbb16c6247c4c566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  44%|####3     | 147M/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d657423fbd4d2788ed57eaab0d4a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  94%|#########3| 461M/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amitm\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55f2a039a3049e0929d24ca62aa31ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stable Diffusion is ready!\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Model ID (Stable Diffusion v1.4)\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "# Load the model (will download if not found)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe.to(device)\n",
    "\n",
    "print(\"âœ… Stable Diffusion is ready!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
